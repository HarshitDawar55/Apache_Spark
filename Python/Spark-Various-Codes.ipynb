{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Work by `Mr. Harshit Dawar`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required Libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the required configuration for the Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Stream Processing Assignment\").setMaster(\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDD and Performing Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Harshit\", \"Raj\", \"Rishu\", \"Alex\", \"Paul\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = sc.parallelize(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harshit', 'Raj', 'Rishu', 'Alex', 'Paul']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the Values in the RDD\n",
    "names.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 1: Map\n",
    "count = names.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harshit', 1), ('Raj', 1), ('Rishu', 1), ('Alex', 1), ('Paul', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 2: Filter\n",
    "Initials = names.filter(lambda x: x[0] == \"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harshit']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Initials.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 3: FlatMap\n",
    "FMCount = names.flatMap(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harshit', 1, 'Raj', 1, 'Rishu', 1, 'Alex', 1, 'Paul', 1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FMCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harshit', 1), ('Raj', 1), ('Rishu', 1), ('Alex', 1), ('Paul', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation 4: reduceByKey\n",
    "count.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 5: Union\n",
    "newNames = sc.parallelize([\"Dawar\", \"Oberoi\", \"katy\", \"kelly kelly\", \"Rajveer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dawar',\n",
       " 'Oberoi',\n",
       " 'katy',\n",
       " 'kelly kelly',\n",
       " 'Rajveer',\n",
       " 'Harshit',\n",
       " 'Raj',\n",
       " 'Rishu',\n",
       " 'Alex',\n",
       " 'Paul']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newNames.union(names).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions on RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harshit', 'Raj', 'Rishu', 'Alex', 'Paul']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action 1: collect()\n",
    "names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harshit', 'Raj', 'Rishu']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action 2: take(number of elements)\n",
    "names.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harshit'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action 3: first()\n",
    "names.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harshit Raj Rishu Alex Paul'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action 4: reduce()\n",
    "names.reduce(lambda a, b: a + \" \" + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action 5: count()\n",
    "names.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Harshit\", \"Raj\", \"Rishu\", \"Alex\", \"Paul\",\n",
    "        \"Harshit\", \"Raj\", \"Rishu\", \"Alex\", \"Paul\",\n",
    "        \"Harshit\", \"Raj\", \"Rishu\", \"Alex\", \"Paul\",\n",
    "        \"Harshit\", \"Raj\", \"Rishu\", \"Alex\", \"Paul\",\n",
    "        \"Harshit\", \"Raj\", \"Rishu\", \"Alex\", \"Paul\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harshit', 5), ('Raj', 5), ('Rishu', 5), ('Alex', 5), ('Paul', 5)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(data).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframes from RDD & Vice-Versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to create Dataframes from Spark\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame = spark.createDataFrame(names.map(lambda x: x.split(\",\"))).toDF(\"Names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  Names|\n",
      "+-------+\n",
      "|Harshit|\n",
      "|    Raj|\n",
      "|  Rishu|\n",
      "|   Alex|\n",
      "|   Paul|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Names='Harshit'),\n",
       " Row(Names='Raj'),\n",
       " Row(Names='Rishu'),\n",
       " Row(Names='Alex'),\n",
       " Row(Names='Paul')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Dataframes into RDD\n",
    "DataFrame.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating table in SparkSQL & Displaying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Image_Name\", StringType(), nullable = False),\n",
    "    StructField(\"SeverityLevel\", IntegerType(), nullable = False)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"file:///Users/harshitdawar/Downloads/Sem 7/Stream Processing Lab/trainLabels.csv\",\n",
    "                         header=True,\n",
    "                         schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|Image_Name|SeverityLevel|\n",
      "+----------+-------------+\n",
      "|   10_left|            0|\n",
      "|  10_right|            0|\n",
      "|   13_left|            0|\n",
      "|  13_right|            0|\n",
      "|   15_left|            1|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|SeverityLevel|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            0|\n",
      "|            0|\n",
      "|            0|\n",
      "|            1|\n",
      "|            2|\n",
      "|            4|\n",
      "|            4|\n",
      "|            0|\n",
      "|            1|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"SeverityLevel\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Finished!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
